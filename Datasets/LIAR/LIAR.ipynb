{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "558d99bb",
   "metadata": {},
   "source": [
    "# LIAR Dataset\n",
    "#### 12k+ entries of authentic, real-world short statements from various contexts with diverse speakers and topics\n",
    "https://paperswithcode.com/dataset/liar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693bd53c",
   "metadata": {},
   "source": [
    "#### Neccessery imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5d66543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8226c4",
   "metadata": {},
   "source": [
    "## 1. Loading the training, testing and validation datasets and merging into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b3d6ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size od df:  (12791, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>json</th>\n",
       "      <th>claim_veracity</th>\n",
       "      <th>claim</th>\n",
       "      <th>topics</th>\n",
       "      <th>speaker</th>\n",
       "      <th>job</th>\n",
       "      <th>state</th>\n",
       "      <th>political_party</th>\n",
       "      <th>credit_history_1</th>\n",
       "      <th>credit_history_2</th>\n",
       "      <th>credit_history_3</th>\n",
       "      <th>credit_history_4</th>\n",
       "      <th>credit_history_5</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2635.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>dwayne-bohac</td>\n",
       "      <td>State representative</td>\n",
       "      <td>Texas</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a mailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10540.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>energy,history,job-accomplishments</td>\n",
       "      <td>scott-surovell</td>\n",
       "      <td>State delegate</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>democrat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a floor speech.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         json claim_veracity  \\\n",
       "0   2635.json          false   \n",
       "1  10540.json      half-true   \n",
       "\n",
       "                                               claim  \\\n",
       "0  Says the Annies List political group supports ...   \n",
       "1  When did the decline of coal start? It started...   \n",
       "\n",
       "                               topics         speaker                   job  \\\n",
       "0                            abortion    dwayne-bohac  State representative   \n",
       "1  energy,history,job-accomplishments  scott-surovell        State delegate   \n",
       "\n",
       "      state political_party  credit_history_1  credit_history_2  \\\n",
       "0     Texas      republican               0.0               1.0   \n",
       "1  Virginia        democrat               0.0               0.0   \n",
       "\n",
       "   credit_history_3  credit_history_4  credit_history_5          context  \n",
       "0               0.0               0.0               0.0         a mailer  \n",
       "1               1.0               1.0               0.0  a floor speech.  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIAR_train_df = pd.read_csv('Initial_datasets/train.csv', delimiter='\\t', header=None, \n",
    "                            index_col=False, low_memory=False)\n",
    "LIAR_test_df = pd.read_csv('Initial_datasets/test.csv', delimiter='\\t', header=None, \n",
    "                           index_col=False, low_memory=False)\n",
    "LIAR_valid_df = pd.read_csv('Initial_datasets/valid.csv', header=None, index_col=False, low_memory=False)\n",
    "\n",
    "LIAR_df = pd.concat([LIAR_train_df, LIAR_test_df, LIAR_valid_df], ignore_index=True)\n",
    "LIAR_df.set_axis(['json', 'claim_veracity', 'claim', 'topics', 'speaker', 'job', 'state', \n",
    "                  'political_party', 'credit_history_1', 'credit_history_2', 'credit_history_3', \n",
    "                  'credit_history_4', 'credit_history_5', 'context'], axis=1, inplace=True)\n",
    "print(\"size od df: \", LIAR_df.shape)\n",
    "LIAR_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4114703b",
   "metadata": {},
   "source": [
    "## 2. Doping useless columns\n",
    "Dropping credit history as it's very specific for this dataset and won't be useful in further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00e7d89d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_veracity</th>\n",
       "      <th>claim</th>\n",
       "      <th>topics</th>\n",
       "      <th>speaker</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>false</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>dwayne-bohac</td>\n",
       "      <td>a mailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>half-true</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>energy,history,job-accomplishments</td>\n",
       "      <td>scott-surovell</td>\n",
       "      <td>a floor speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "      <td>foreign-policy</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>Denver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>false</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "      <td>health-care</td>\n",
       "      <td>blog-posting</td>\n",
       "      <td>a news release</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>half-true</td>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "      <td>economy,jobs</td>\n",
       "      <td>charlie-crist</td>\n",
       "      <td>an interview on CNN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  claim_veracity                                              claim  \\\n",
       "0          false  Says the Annies List political group supports ...   \n",
       "1      half-true  When did the decline of coal start? It started...   \n",
       "2    mostly-true  Hillary Clinton agrees with John McCain \"by vo...   \n",
       "3          false  Health care reform legislation is likely to ma...   \n",
       "4      half-true  The economic turnaround started at the end of ...   \n",
       "\n",
       "                               topics         speaker              context  \n",
       "0                            abortion    dwayne-bohac             a mailer  \n",
       "1  energy,history,job-accomplishments  scott-surovell      a floor speech.  \n",
       "2                      foreign-policy    barack-obama               Denver  \n",
       "3                         health-care    blog-posting       a news release  \n",
       "4                        economy,jobs   charlie-crist  an interview on CNN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIAR_df.drop(['json', 'political_party', 'job', 'state', 'credit_history_1', 'credit_history_2', \n",
    "              'credit_history_3', 'credit_history_4', 'credit_history_5'], axis=1, inplace=True)\n",
    "LIAR_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a32447",
   "metadata": {},
   "source": [
    "## 3. Columns formating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c042222",
   "metadata": {},
   "source": [
    "### 3.1 Topics\n",
    "Changing topics to be an array of topics not a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "048b702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIAR_df['topics'] = LIAR_df['topics'].str.split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df37106",
   "metadata": {},
   "source": [
    "### 3.2 Speaker\n",
    "ex: from hilary-clinton to Hilary Clinton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b79a4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIAR_df['speaker'] = LIAR_df['speaker'].str.replace(\"-\", \" \").str.title()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737c72d1",
   "metadata": {},
   "source": [
    "### 3.3 Claims\n",
    "- There are many claims in this dataset that paraphrase what speaker said ex: \"Says John McCain has done nothing to help the vets.\"\n",
    "\n",
    "- Because I am more focused on the actual claim rather than who said it, whenever I can I want to eliminate the speaker ex: \"John McCain has done nothing to help the vets.\"\n",
    "\n",
    "- Sometimes however if the claim is personal I add the speaker before the claim to form a sentence such as: \"Donald Trump says John McCain has done nothing to help the vets.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65e9b208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences that have his/him/he/hes/she/shes/her/hers will have a speaker added to them \n",
    "personal_list = (' his ', ' him ', ' he ', ' hes ', ' she ', ' shes ', ' her ', ' hers ')\n",
    "pattern = '|'.join(personal_list)\n",
    "LIAR_df.loc[((LIAR_df['claim'].str.startswith('Says')) & LIAR_df['claim'].str.contains(pattern)), \n",
    "            'claim'] = LIAR_df['speaker'].str[:] + \" \" + LIAR_df['claim'].str[:]\n",
    "\n",
    "# Deleting Says that\n",
    "LIAR_df.loc[LIAR_df['claim'].str.startswith('Says that'), \"claim\"] = LIAR_df['claim'].str[10:]\n",
    "\n",
    "# Deleting Says a\n",
    "LIAR_df.loc[LIAR_df['claim'].str.startswith('Says a '), \"claim\"] = LIAR_df['claim'].str[7:]\n",
    "\n",
    "# Deleting Says the\n",
    "LIAR_df.loc[LIAR_df['claim'].str.startswith('Says the '), \"claim\"] = LIAR_df['claim'].str[5:]\n",
    "\n",
    "# Deleting Says the\n",
    "LIAR_df.loc[LIAR_df['claim'].str.startswith('Says '), \"claim\"] = LIAR_df['claim'].str[5:]\n",
    "\n",
    "# Deleting clais that start with \"On \" and don't contain ':' -> They usually don't have any claim\n",
    "# Example claim: 'On mandating health care coverage' - It doesn't tell us anything\n",
    "LIAR_df.drop(LIAR_df.loc[(LIAR_df['claim'].str.lower().str.startswith('on ')) & \n",
    "                         (LIAR_df['claim'].str.contains(':') == False)].index, inplace=True)\n",
    "\n",
    "# Adding the speaker to the remaining claims that start with 'On '\n",
    "LIAR_df.loc[LIAR_df['claim'].str.startswith('On '), 'claim'] = LIAR_df['speaker'].str[:] + \" o\" + LIAR_df['claim'].str[1:]\n",
    "\n",
    "# Capitalise the claims\n",
    "LIAR_df['claim'] = LIAR_df['claim'].str[0].str.capitalize() + LIAR_df['claim'].str[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c75fffcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_veracity</th>\n",
       "      <th>claim</th>\n",
       "      <th>topics</th>\n",
       "      <th>speaker</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>barely-true</td>\n",
       "      <td>Donald Trump on the VA: Over 300,000 veterans ...</td>\n",
       "      <td>[health-care, veterans]</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>a speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>mostly-true</td>\n",
       "      <td>It is a commitment voters take very seriously:...</td>\n",
       "      <td>[elections, taxes]</td>\n",
       "      <td>Americans Tax Reform</td>\n",
       "      <td>a news release</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>half-true</td>\n",
       "      <td>Hypocrisy at the Clinton Foundation: Top male ...</td>\n",
       "      <td>[candidates-biography, women, workers]</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>an Instagram post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>true</td>\n",
       "      <td>You know we can't just pull out now... The tru...</td>\n",
       "      <td>[iraq]</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>CNN/YouTube debate in Charleston, S.C.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>pants-fire</td>\n",
       "      <td>Sheila Jackson Lee of Texas said: Hey, all you...</td>\n",
       "      <td>[candidates-biography, diversity, campaign-adv...</td>\n",
       "      <td>Facebook Posts</td>\n",
       "      <td>a meme supposedly quoting Sheila Jackson Lee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    claim_veracity                                              claim  \\\n",
       "149    barely-true  Donald Trump on the VA: Over 300,000 veterans ...   \n",
       "188    mostly-true  It is a commitment voters take very seriously:...   \n",
       "267      half-true  Hypocrisy at the Clinton Foundation: Top male ...   \n",
       "357           true  You know we can't just pull out now... The tru...   \n",
       "375     pants-fire  Sheila Jackson Lee of Texas said: Hey, all you...   \n",
       "\n",
       "                                                topics               speaker  \\\n",
       "149                            [health-care, veterans]          Donald Trump   \n",
       "188                                 [elections, taxes]  Americans Tax Reform   \n",
       "267             [candidates-biography, women, workers]          Donald Trump   \n",
       "357                                             [iraq]             Joe Biden   \n",
       "375  [candidates-biography, diversity, campaign-adv...        Facebook Posts   \n",
       "\n",
       "                                          context  \n",
       "149                                     a speech.  \n",
       "188                                a news release  \n",
       "267                             an Instagram post  \n",
       "357        CNN/YouTube debate in Charleston, S.C.  \n",
       "375  a meme supposedly quoting Sheila Jackson Lee  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LIAR_df[(LIAR_df['claim'].str.lower().str.startswith('on ')) & (LIAR_df['claim'].str.len() > 40)]\n",
    "LIAR_df[(LIAR_df['claim'].str.lower().str.contains('on')) & (LIAR_df['claim'].str.contains(':'))].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca4b6a7",
   "metadata": {},
   "source": [
    "### 3.4 claim_veracity\n",
    "- TRUE: true, mostly-true\n",
    "- FALSE: false, pants-fire, barely-true\n",
    "- half-true data entries will go through google search verifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19719f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(LIAR_df, x='claim_veracity').update_xaxes(categoryarray=['pants-fire', 'false', 'barely-true', 'half-true', 'mostly-true', 'true'])\n",
    "fig.update_layout(bargap=0.2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14bde415",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [LIAR_df['claim_veracity'].eq('true'),\n",
    "              LIAR_df['claim_veracity'].eq('mostly-true'),\n",
    "              LIAR_df['claim_veracity'].eq('TRUE'),\n",
    "              LIAR_df['claim_veracity'].eq('pants-fire'),\n",
    "              LIAR_df['claim_veracity'].eq('false'),\n",
    "              LIAR_df['claim_veracity'].eq('barely-true'),\n",
    "              LIAR_df['claim_veracity'].eq('FALSE')]\n",
    "choices = [True, True, True, False, False, False, False]\n",
    "LIAR_df['claim_veracity'] = np.select(conditions, choices, default = LIAR_df['claim_veracity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3ca99c",
   "metadata": {},
   "source": [
    "Half-true claims will be a grey area of this dataset as they will be the hardest to categorise and requires the most amount of time to investigate. It's also the biggest category so adding all to either \"false\" or \"true\" will shift the balance quite dramatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacf3f53",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = px.histogram(LIAR_df, x='claim_veracity').update_xaxes(categoryarray=['pants-fire', 'false', 'barely-true', 'half-true', 'mostly-true', 'true'])\n",
    "fig.update_layout(bargap=0.2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ffd0aa",
   "metadata": {},
   "source": [
    "## 4. Inspecting characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d486b8e",
   "metadata": {},
   "source": [
    "### 4.1 Topics distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252ba352",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Replacing NaN with empty arrays\n",
    "isna = LIAR_df['topics'].isna()\n",
    "LIAR_df.loc[isna, 'topics'] = pd.Series([[]] * isna.sum()).values\n",
    "\n",
    "topics_occurances = LIAR_df.topics.sum()\n",
    "topics_dict = {i:topics_occurances.count(i) for i in set(topics_occurances)}\n",
    "\n",
    "fig = px.pie(LIAR_df, values=list(topics_dict.values()), names=list(topics_dict.keys()))\n",
    "fig.update_traces(textposition='inside', textinfo='value')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b065621b",
   "metadata": {},
   "source": [
    "We can clearly see that this dataset is diverse and doesn't focus on one specific topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbf1345",
   "metadata": {},
   "source": [
    "### 4.1 Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9f0dc6",
   "metadata": {},
   "source": [
    "#### Context modification\n",
    "The context was very specfic so this changes the context to more be more general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa72f1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5047\n"
     ]
    }
   ],
   "source": [
    "print(len(LIAR_df['context'].unique())) #Over 5k possible contexts...\n",
    "\n",
    "LIAR_df['context'] = LIAR_df['context'].fillna('None')\n",
    "\n",
    "# Array that define change in context given certain keywords\n",
    "speech_context = [['speech', 'senate', 'state', 'house', 'floor', 'meeting', \n",
    "                   'answer', 'commentary', 'presentation', 'hearing', 'opinion', \n",
    "                   'talk', 'appearance', 'interview', 'debate', 'radio', 'rally', \n",
    "                   'remark', 'conference', 'discussion', 'comment', 'town', 'response'], 'speech']\n",
    "add_context = [['ad', 'mailer', 'flier', 'billboard', 'commercial', 'flyer', 'brochure', 'campaign'], 'ad']\n",
    "social_media_context = [['twitter', 'tweet', 'facebook', 'post', 'social media', \n",
    "                     'media', 'web', 'forum', 'blog', 'internet', 'video', 'message'], 'social media']\n",
    "news_context = [['press', 'news', 'website', 'cnn', 'tv', 'release', 'article', 'segment', \n",
    "             'abc', 'nbc', 'television', 'fox', 'msnbc', 'editiorial', 'column', 'op-ed'], 'news']\n",
    "writing_context = [['book', 'email', 'e-mail', 'letter', 'report', 'autobiography', 'report', 'petition', 'survey', 'statement'], 'writing']\n",
    "\n",
    "# Changing context given keywords\n",
    "contexts_array = [speech_context, add_context, social_media_context, news_context, writing_context]\n",
    "for context in contexts_array:\n",
    "    context_regex = \"|\".join(context[0])\n",
    "    LIAR_df.loc[LIAR_df['context'].str.lower().str.contains(context_regex), 'context'] = context[1]\n",
    "    \n",
    "    \n",
    "# Other contexts are treated as undefined (they are very vague)\n",
    "possible_contexts = ['speech', 'ad', 'social media', 'news', 'writing']\n",
    "possible_contexts_regex = \"|\".join(possible_contexts)\n",
    "LIAR_df.loc[~LIAR_df['context'].str.lower().str.contains(possible_contexts_regex), 'context'] = 'Undefined'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86ff9b3",
   "metadata": {},
   "source": [
    "#### New Context Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705c8fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(LIAR_df, names = 'context', color = 'context', color_discrete_sequence=px.colors.qualitative.Pastel)\n",
    "fig.update_traces(text = LIAR_df['context'].value_counts(), textinfo = 'label+percent')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc249e21",
   "metadata": {},
   "source": [
    "## 5. Examining Google-verified Half-True entries\n",
    "Half-true entries were fed into google_scraper, where the average levensthein distance between the claim and the google query results was calculated. This allows to more in-depth analysis and observations on whereas a half-true claim should be considered true or false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25da2adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIAR_df = LIAR_df[LIAR_df['claim_veracity'] != 'half-true']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad855dbc",
   "metadata": {},
   "source": [
    "### 5.1. Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6db47a6f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "pd.options.display.max_colwidth = 200\n",
    "\n",
    "LIAR_score = pd.read_csv('../../Google_Scraper/LIAR_verified/LIAR_half_true_score.csv', \n",
    "                         index_col=0, low_memory=False)\n",
    "\n",
    "# Delete the entries with leven_score = 0\n",
    "LIAR_score = LIAR_score.drop(LIAR_score.loc[LIAR_score['leven__score'] <= 0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "453dbba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting entries that were omitted before\n",
    "LIAR_score.drop(LIAR_score.loc[(LIAR_score['claim'].str.lower().str.startswith('on')) & \n",
    "                               (LIAR_score['claim'].str.contains(':') == False)].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb60809",
   "metadata": {},
   "source": [
    "### 5.2. Adjusting levenshtein distance score\n",
    "Adjusting leven_score to account for the claim character count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a41d0dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LIAR_score['claim_count'] = LIAR_score['claim'].str.len()\n",
    "LIAR_score['adjusted_leven'] = LIAR_score['leven__score'] / LIAR_score['claim_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bf8452",
   "metadata": {},
   "source": [
    "### 5.3. Splitting into groups based on sentence length\n",
    "From each group the 50% of half-true entries that had the best (smallest) adjusted_leven will be considered as true and false vice-versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c526bda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 different ranges of characters length that split the dataset into 4 groups: \n",
      "\n",
      " [(94.75, 170.5], (18.697, 94.75], (170.5, 246.25], (246.25, 322.0]]\n",
      "Categories (4, interval[float64]): [(18.697, 94.75] < (94.75, 170.5] < (170.5, 246.25] < (246.25, 322.0]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_veracity</th>\n",
       "      <th>claim</th>\n",
       "      <th>leven__score</th>\n",
       "      <th>claim_count</th>\n",
       "      <th>adjusted_leven</th>\n",
       "      <th>char_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>half-true</td>\n",
       "      <td>Harvard Study Finds States With Most Gun Laws Have Fewest Gun Deaths.</td>\n",
       "      <td>37.222222</td>\n",
       "      <td>69</td>\n",
       "      <td>0.539452</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>half-true</td>\n",
       "      <td>People (are) paying more in taxes than they will for food, housing and clothing combined.</td>\n",
       "      <td>48.272727</td>\n",
       "      <td>89</td>\n",
       "      <td>0.542390</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>half-true</td>\n",
       "      <td>Americans will spend more on taxes in 2015 than on food, clothing and housing combined.</td>\n",
       "      <td>47.333333</td>\n",
       "      <td>87</td>\n",
       "      <td>0.544061</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>half-true</td>\n",
       "      <td>Undocumented immigrantspay $12 billion a year into Social Security.</td>\n",
       "      <td>37.400000</td>\n",
       "      <td>67</td>\n",
       "      <td>0.558209</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2029</th>\n",
       "      <td>half-true</td>\n",
       "      <td>Americans will spend more on taxes in 2014 than they will on food, clothing and housing combined.</td>\n",
       "      <td>56.545455</td>\n",
       "      <td>97</td>\n",
       "      <td>0.582943</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     claim_veracity  \\\n",
       "1805      half-true   \n",
       "1436      half-true   \n",
       "988       half-true   \n",
       "812       half-true   \n",
       "2029      half-true   \n",
       "\n",
       "                                                                                                  claim  \\\n",
       "1805                              Harvard Study Finds States With Most Gun Laws Have Fewest Gun Deaths.   \n",
       "1436          People (are) paying more in taxes than they will for food, housing and clothing combined.   \n",
       "988             Americans will spend more on taxes in 2015 than on food, clothing and housing combined.   \n",
       "812                                 Undocumented immigrantspay $12 billion a year into Social Security.   \n",
       "2029  Americans will spend more on taxes in 2014 than they will on food, clothing and housing combined.   \n",
       "\n",
       "      leven__score  claim_count  adjusted_leven char_category  \n",
       "1805     37.222222           69        0.539452             0  \n",
       "1436     48.272727           89        0.542390             0  \n",
       "988      47.333333           87        0.544061             0  \n",
       "812      37.400000           67        0.558209             0  \n",
       "2029     56.545455           97        0.582943             1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"4 different ranges of characters length that split the dataset into 4 groups: \\n\\n\", \n",
    "      pd.cut(LIAR_score['claim_count'], 4).unique())\n",
    "\n",
    "LIAR_score['char_category'] = pd.cut(LIAR_score['claim_count'], 4, labels=[0, 1, 2, 3])\n",
    "LIAR_score.sort_values('adjusted_leven').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ebfab9",
   "metadata": {},
   "source": [
    "### 5.4. Taking 50% of best/worse entries from each group to be true/false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa5b8210",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs=[]\n",
    "\n",
    "for category in [0,1,2,3]:\n",
    "    category_df = LIAR_score[LIAR_score['char_category'] == category].sort_values('adjusted_leven')\n",
    "    half_index = int(len(category_df) * 0.5)\n",
    "    \n",
    "    true_df = category_df.iloc[:half_index]\n",
    "    true_df['claim_veracity'] = True\n",
    "    dfs.append(true_df)\n",
    "    \n",
    "    false_df = category_df.iloc[half_index:]\n",
    "    false_df['claim_veracity'] = False\n",
    "    dfs.append(false_df)\n",
    "    \n",
    "half_true_df = pd.concat(dfs)\n",
    "LIAR_Final = pd.concat([half_true_df, LIAR_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2355190e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_veracity</th>\n",
       "      <th>claim</th>\n",
       "      <th>leven__score</th>\n",
       "      <th>claim_count</th>\n",
       "      <th>adjusted_leven</th>\n",
       "      <th>char_category</th>\n",
       "      <th>topics</th>\n",
       "      <th>speaker</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>True</td>\n",
       "      <td>Harvard Study Finds States With Most Gun Laws Have Fewest Gun Deaths.</td>\n",
       "      <td>37.222222</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0.539452</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>True</td>\n",
       "      <td>People (are) paying more in taxes than they will for food, housing and clothing combined.</td>\n",
       "      <td>48.272727</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.542390</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     claim_veracity  \\\n",
       "1805           True   \n",
       "1436           True   \n",
       "\n",
       "                                                                                          claim  \\\n",
       "1805                      Harvard Study Finds States With Most Gun Laws Have Fewest Gun Deaths.   \n",
       "1436  People (are) paying more in taxes than they will for food, housing and clothing combined.   \n",
       "\n",
       "      leven__score  claim_count  adjusted_leven char_category topics speaker  \\\n",
       "1805     37.222222         69.0        0.539452             0    NaN     NaN   \n",
       "1436     48.272727         89.0        0.542390             0    NaN     NaN   \n",
       "\n",
       "     context  \n",
       "1805     NaN  \n",
       "1436     NaN  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIAR_Final.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba09c598",
   "metadata": {},
   "source": [
    "## 6. Saving modified dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e22e085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unncessery columns, renamining and rearranging remaining one\n",
    "LIAR_Final = LIAR_Final.sample(frac=1).reset_index(drop=True).drop(['leven__score', 'claim_count', 'adjusted_leven', 'char_category', 'topics', 'speaker', 'context'], axis=1)\n",
    "LIAR_Final = LIAR_Final[['claim', 'claim_veracity']]\n",
    "LIAR_Final['claim_veracity'] = LIAR_Final['claim_veracity'].astype(int)\n",
    "\n",
    "LIAR_Final.to_csv('LIAR_Final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd634d11",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
