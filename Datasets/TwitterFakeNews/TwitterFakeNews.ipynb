{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ff19595",
   "metadata": {},
   "source": [
    "# TwitterFakeNews\n",
    "#### Large scale dataset of 200k+ tweets that were labelled automatically to be true/false. Tweets that are issued by accounts known for spreading false news are considered false (all of them) and vice-versa for trustworthy accounts\n",
    "\n",
    "https://www.mdpi.com/1999-5903/13/5/114"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb8762e",
   "metadata": {},
   "source": [
    "#### Neccessery imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb3d650d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "pd.options.display.max_colwidth = 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9c035a",
   "metadata": {},
   "source": [
    "## 1. Loading the dataset\n",
    "tweet_fake - 1 (Fake), 0 (True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4eb5198",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet__retweet_count</th>\n",
       "      <th>tweet__favorite_count</th>\n",
       "      <th>tweet__fake</th>\n",
       "      <th>tweet__sent_tokenized_text</th>\n",
       "      <th>tweet__text</th>\n",
       "      <th>tweet__nr_of_sentences</th>\n",
       "      <th>tweet__tokenized_um_url_removed</th>\n",
       "      <th>tweet__nr_of_hashtags</th>\n",
       "      <th>tweet__contains_hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"Joe Buck ruined Brooks Koepka's U.S. Open kiss with a super-awkward mistake \"]</td>\n",
       "      <td>Joe Buck ruined Brooks Koepka's U.S. Open kiss with a super-awkward mistake https://t.co/UO1GcIUPU6</td>\n",
       "      <td>1</td>\n",
       "      <td>['Joe', 'Buck', 'ruined', 'Brooks', \"Koepka's\", 'U', '.', 'S', '.', 'Open', 'kiss', 'with', 'a', 'super-awkward', 'mistake']</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>['Two engaged doctors found bound and slain in luxury Boston penthouse.', 'Man in custody after shootout. ']</td>\n",
       "      <td>Two engaged doctors found bound and slain in luxury Boston penthouse. Man in custody after shootout. https://t.co/A22ivfzonQ</td>\n",
       "      <td>2</td>\n",
       "      <td>['Two', 'engaged', 'doctors', 'found', 'bound', 'and', 'slain', 'in', 'luxury', 'Boston', 'penthouse', '.', 'Man', 'in', 'custody', 'after', 'shootout', '.']</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['DROP EVERYTHING and watch these four urgent, must-see #Documentries  ']</td>\n",
       "      <td>DROP EVERYTHING and watch these four urgent, must-see #Documentries https://t.co/TiFhU40nWq https://t.co/1dk8PZWFYC</td>\n",
       "      <td>1</td>\n",
       "      <td>['drop', 'everything', 'and', 'watch', 'these', 'four', 'urgent', ',', 'must-see']</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet__retweet_count  tweet__favorite_count  tweet__fake  \\\n",
       "0                    32                     71            0   \n",
       "1                    67                     55            0   \n",
       "2                     0                      1            1   \n",
       "\n",
       "                                                                                     tweet__sent_tokenized_text  \\\n",
       "0                              [\"Joe Buck ruined Brooks Koepka's U.S. Open kiss with a super-awkward mistake \"]   \n",
       "1  ['Two engaged doctors found bound and slain in luxury Boston penthouse.', 'Man in custody after shootout. ']   \n",
       "2                                     ['DROP EVERYTHING and watch these four urgent, must-see #Documentries  ']   \n",
       "\n",
       "                                                                                                                    tweet__text  \\\n",
       "0                           Joe Buck ruined Brooks Koepka's U.S. Open kiss with a super-awkward mistake https://t.co/UO1GcIUPU6   \n",
       "1  Two engaged doctors found bound and slain in luxury Boston penthouse. Man in custody after shootout. https://t.co/A22ivfzonQ   \n",
       "2           DROP EVERYTHING and watch these four urgent, must-see #Documentries https://t.co/TiFhU40nWq https://t.co/1dk8PZWFYC   \n",
       "\n",
       "   tweet__nr_of_sentences  \\\n",
       "0                       1   \n",
       "1                       2   \n",
       "2                       1   \n",
       "\n",
       "                                                                                                                                 tweet__tokenized_um_url_removed  \\\n",
       "0                                   ['Joe', 'Buck', 'ruined', 'Brooks', \"Koepka's\", 'U', '.', 'S', '.', 'Open', 'kiss', 'with', 'a', 'super-awkward', 'mistake']   \n",
       "1  ['Two', 'engaged', 'doctors', 'found', 'bound', 'and', 'slain', 'in', 'luxury', 'Boston', 'penthouse', '.', 'Man', 'in', 'custody', 'after', 'shootout', '.']   \n",
       "2                                                                             ['drop', 'everything', 'and', 'watch', 'these', 'four', 'urgent', ',', 'must-see']   \n",
       "\n",
       "   tweet__nr_of_hashtags  tweet__contains_hashtags  \n",
       "0                      0                     False  \n",
       "1                      0                     False  \n",
       "2                      1                      True  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "picked_columns = ['tweet__text', 'tweet__contains_hashtags', 'tweet__nr_of_hashtags', 'tweet__sent_tokenized_text', 'tweet__tokenized_um_url_removed', 'tweet__nr_of_sentences', 'tweet__fake', 'tweet__retweet_count', 'tweet__favorite_count']\n",
    "Auto_df = pd.read_csv('Initial_datasets/data.csv', sep=';', usecols = picked_columns , low_memory=False)\n",
    "\n",
    "Auto_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b588a08",
   "metadata": {},
   "source": [
    "## 2. Claim Insepctions & Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76b83a1",
   "metadata": {},
   "source": [
    "### 2.1. URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c24a024",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188773\n"
     ]
    }
   ],
   "source": [
    "# The one that start with a link are dropped (41 entries)\n",
    "Auto_df = Auto_df.drop(Auto_df.loc[Auto_df['tweet__text'].str.lower().str.startswith('http')].index)\n",
    "\n",
    "#The ones that have link in the middle are also dropped (8500) (too risky - most of them are faulty/weird)\n",
    "Auto_df = Auto_df.drop(Auto_df.loc[Auto_df['tweet__text'].str.lower().str.split().str[-1].str.contains('http') == False].index)\n",
    "\n",
    "#The ones that have a link at the end (180.000+) are having their link\\links deleted\n",
    "f = lambda x: ' '.join([item for item in x.split() if 'http' not in item])\n",
    "Auto_df[\"tweet__text\"] = Auto_df[\"tweet__text\"].apply(f)\n",
    "\n",
    "print(Auto_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ee1c79",
   "metadata": {},
   "source": [
    "### 2.2. Hashtags - Dropping hashtags\n",
    "Because the dataset is big we can drop the hashtags and still end up with a very big dataset. The hashtag entries are not reliable, hence the decision to not examine them any further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2fbed68",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of entries with hashtags:  25098\n"
     ]
    }
   ],
   "source": [
    "print(\"size of entries with hashtags: \", Auto_df[Auto_df['tweet__contains_hashtags'] == True].shape[0])\n",
    "hash_df = Auto_df[Auto_df['tweet__contains_hashtags'] == True]\n",
    "\n",
    "Auto_df = Auto_df.drop(Auto_df[Auto_df['tweet__contains_hashtags'] == True].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da434539",
   "metadata": {},
   "source": [
    "### 2.3. Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95bf0b6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dropping duplicate entries (over 25.0000 rows)\n",
    "Auto_df = Auto_df.drop_duplicates(subset='tweet__text', keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b24632",
   "metadata": {},
   "source": [
    "### 2.4. @ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a062348",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Droping entries with mentions (@) as there is no real way to transorm them to normal text (around 15.000)\n",
    "Auto_df = Auto_df.drop(Auto_df.loc[Auto_df['tweet__text'].str.lower().str.contains('@')].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb31245",
   "metadata": {},
   "source": [
    "### 2.5. Other Special Characters\n",
    "- &amp\n",
    "- ;\n",
    "- non ascii characters (like emojis for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "679a2999",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop entries containing &amp (1500)\n",
    "Auto_df = Auto_df.drop(Auto_df[Auto_df['tweet__text'].str.contains('&amp')].index)\n",
    "Auto_df = Auto_df.replace(';',',', regex=True)\n",
    "Auto_df['tweet__text'] = Auto_df['tweet__text'].astype(str).apply(lambda x: x.encode('ascii', 'ignore').decode('ascii')) # remove characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5249bc",
   "metadata": {},
   "source": [
    "### 2.6. Nr of Sentences\n",
    "- Deleting everything that is over 2 sentences long (around 1500 entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cf76d65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Auto_df = Auto_df.drop(Auto_df[Auto_df['tweet__nr_of_sentences'] > 2].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d748e2b",
   "metadata": {},
   "source": [
    "### 2.7. Tweet Favorite/Retweet Count - Maybe take only the ones that are above some tweet fav count\n",
    "- Droping the tweets that have 0 fav and retweet count (Not really worth checking them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4885186f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Auto_df = Auto_df.drop(Auto_df[Auto_df['tweet__favorite_count'] == 0].index)\n",
    "Auto_df = Auto_df.drop(Auto_df[Auto_df['tweet__retweet_count'] == 0].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09af70ff",
   "metadata": {},
   "source": [
    "### 2.8. VIDEO entries\n",
    "- Many entries have '- video' at the end of their claims. Drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb34f92b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Auto_df = Auto_df.drop(Auto_df[Auto_df['tweet__text'].str.contains('- video')].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9debad2c",
   "metadata": {},
   "source": [
    "## 3. True/Fake Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a654729",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = px.histogram(Auto_df, x='tweet__fake')\n",
    "fig.update_layout(bargap=0.2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7209847",
   "metadata": {},
   "source": [
    "##  4. Saving new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66eaaa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping unncessery columns\n",
    "Auto_df.drop(['tweet__retweet_count', 'tweet__favorite_count', 'tweet__sent_tokenized_text', 'tweet__nr_of_sentences', 'tweet__tokenized_um_url_removed', 'tweet__nr_of_hashtags', 'tweet__contains_hashtags'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3007db59",
   "metadata": {},
   "source": [
    "- Spliting formatted data into fake and true entries that will be fed to the google verification algorithm, where each entry will get a levensthein distance score based on inputing the claim in google search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f205b112",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "true_df = Auto_df[Auto_df['tweet__fake'] == 0]\n",
    "fake_df = Auto_df[Auto_df['tweet__fake'] == 1]\n",
    "\n",
    "true_df.to_csv('Auto_Format_True.csv', sep=';', encoding='utf-8')\n",
    "fake_df.to_csv('Auto_Format_Fake.csv', sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2d9f805",
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_df.to_csv('TwitterFakeNews_Final.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63681e6f",
   "metadata": {},
   "source": [
    "## 5. Examining Google Verified Data\n",
    "- These datasets went through a \"google verification\" method where each claim was inputted to the google search engine, and the levensthein score between the claim and the results were calculated. The idea is that the entries with higher average levensthein score are not as trustworthy as the ones with lower distance. This suggest that if the query has many similar results in google it should be considered true, and false otherwise. This was done to this noisy dataset as a layer of verification to their automatic labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869cfab5",
   "metadata": {},
   "source": [
    "### 5.1. Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ceb71ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Auto_true_score = pd.read_csv('Formatted_datasets/Auto_Format_True_Score.csv', index_col=0, low_memory=False)\n",
    "Auto_false_score = pd.read_csv('Formatted_datasets/Auto_Format_False_Score.csv', index_col=0, low_memory=False)\n",
    "\n",
    "# Delete the entries with leven_score = 0\n",
    "Auto_true_score = Auto_true_score.drop(Auto_true_score.loc[Auto_true_score['leven__score'] <= 0].index)\n",
    "Auto_false_score = Auto_false_score.drop(Auto_false_score.loc[Auto_false_score['leven__score'] <= 0].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a822ed",
   "metadata": {},
   "source": [
    "### 5.2. Sentence Length correlation to score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e375f3",
   "metadata": {},
   "source": [
    "- For True entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "caa99bbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9132956515728649"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Auto_true_score['tweet__count'] = Auto_true_score['tweet__text'].str.len()\n",
    "Auto_true_score['tweet__count'].corr(Auto_true_score['leven__score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5f267c",
   "metadata": {},
   "source": [
    "- For False entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3550d8a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9053585456499308"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Auto_false_score['tweet__count'] = Auto_false_score['tweet__text'].str.len()\n",
    "Auto_false_score['tweet__count'].corr(Auto_false_score['leven__score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d06f88",
   "metadata": {},
   "source": [
    "As we can see there is a big correaltion between the tweet count and the levesthein score calculated from the google verifier. To adjust for that a new column is created"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3408cbda",
   "metadata": {},
   "source": [
    "### 5.3. Adding Leven_Score/Tweet_Count column\n",
    "- Adjusted Levensthein score to adjust for the length of the tweet. The leven_score alone was biased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "568645ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Auto_true_score['adjusted_leven'] = Auto_true_score['leven__score'] / Auto_true_score['tweet__count']\n",
    "Auto_false_score['adjusted_leven'] = Auto_false_score['leven__score'] / Auto_false_score['tweet__count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b44ca620",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.08080815730036466\n",
      "-0.385536888505841\n"
     ]
    }
   ],
   "source": [
    "print(Auto_true_score['tweet__count'].corr(Auto_true_score['adjusted_leven']))\n",
    "print(Auto_false_score['tweet__count'].corr(Auto_false_score['adjusted_leven']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d85e7c",
   "metadata": {},
   "source": [
    "As we can see strong correlation is now gone, making the results more trustworthy. The lower adjusted_leven, the more chance the tweet__text label is actually true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeb8016",
   "metadata": {},
   "source": [
    "### 5.4 Picking Best Entries Based On Sentence Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e22e320",
   "metadata": {},
   "source": [
    "### - For True entries:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cf58e7",
   "metadata": {},
   "source": [
    "Pick 3500 entries with the best (lowest) adjusted_leven score -> This indicates that is simillarity between the claim and the google search query results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3da7a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Take 3000 best rated entries from all\n",
    "Final_Auto_True = Auto_true_score.sort_values('adjusted_leven').head(3500)\n",
    "Final_Auto_True\n",
    "\n",
    "fig = px.histogram(Final_Auto_True, x='tweet__count', title=\"Best 3500 claims and their characters length\")\n",
    "fig.update_layout(bargap=0.2)\n",
    "fig.show() \n",
    "\n",
    "fig = px.histogram(Auto_true_score, x='tweet__count', title=\"Characters length distribution of all true entries\")\n",
    "fig.update_layout(bargap=0.2)\n",
    "fig.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d7e472",
   "metadata": {},
   "source": [
    "##### Conclusion\n",
    "We can see claims mostly at around 60 characters long, and not much claims that are over 80 characters long. However from the distribution of the whole dataset we can see that big portion of the claims are longer entries with around 100+ characters. Whereas on manual inspection shorter claims appear to be make more sense and be more reliable I decided to include "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94703785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 different ranges of characters length that split the dataset into 4 groups: \n",
      "\n",
      " [(89.75, 118.0], (61.5, 89.75], (33.25, 61.5], (4.887, 33.25]]\n",
      "Categories (4, interval[float64]): [(4.887, 33.25] < (33.25, 61.5] < (61.5, 89.75] < (89.75, 118.0]]\n"
     ]
    }
   ],
   "source": [
    "# Remaining dataset: (droping rows that are already considered valid)\n",
    "Reamaining_df = Auto_true_score.drop(Final_Auto_True.index)\n",
    "\n",
    "print(\"4 different ranges of characters length that split the dataset into 4 groups: \\n\\n\", \n",
    "      pd.cut(Reamaining_df['tweet__count'], 4).unique())\n",
    "\n",
    "Reamaining_df['char__category'] = pd.cut(Reamaining_df['tweet__count'], 4, labels=[0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81292e3c",
   "metadata": {},
   "source": [
    "#### Category Inspection conclusions:\n",
    "- **0 (4 - 33 characters)** -> Entries don't tell anything and **should not** be classified as true or false. They are mostly empty headlines. Do not include\n",
    "- **1 (34 - 61 characters)** -> Entries seem very reliable and the ones with the best score show most potential. Include best 500\n",
    "- **2 (62 - 89 characters)** -> These also show promise and look reliable upon initial inspection. Include best 500\n",
    "- **3 (90 - 118 chatacters)** -> Same with these. Include best 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee787d72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adding additional 1500 enries from categories 1-3 (500 best in each category)\n",
    "for category in [1,2,3]:\n",
    "    top500_df = Reamaining_df[Reamaining_df['char__category'] == category].sort_values('adjusted_leven').head(500)\n",
    "    Final_Auto_True = pd.concat([Final_Auto_True, top500_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5162ff31",
   "metadata": {},
   "source": [
    "### - For Fake entries:\n",
    "For fake entries we will pick claims that achieved the biggest levensthein distance, meaning entries that had the biggest discrepancy between the claim and the google search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f068c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = px.histogram(Auto_false_score, x='tweet__count', title=\"Characters length distribution of all fake entries\")\n",
    "fig.update_layout(bargap=0.2)\n",
    "fig.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799909dd",
   "metadata": {},
   "source": [
    "Upon inspection we can't just simply take the worst values from leven__score or adjusted_leven as the values are biased towards claims with little characters or many characters accordingly. Therefore as with true instances, the entries are grouped by character length beforehand ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fff70d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 different ranges of characters length that split the dataset into 4 groups: \n",
      "\n",
      " [(88.0, 116.0], (32.0, 60.0], (60.0, 88.0], (3.888, 32.0]]\n",
      "Categories (4, interval[float64]): [(3.888, 32.0] < (32.0, 60.0] < (60.0, 88.0] < (88.0, 116.0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"4 different ranges of characters length that split the dataset into 4 groups: \\n\\n\", \n",
    "      pd.cut(Auto_false_score['tweet__count'], 4).unique())\n",
    "\n",
    "Auto_false_score['char__category'] = pd.cut(Auto_false_score['tweet__count'], 4, labels=[0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add47da2",
   "metadata": {},
   "source": [
    "#### Category Inspection conclusions:\n",
    "- **0 (4 - 32 characters)** -> Entries don't tell anything and **should not** be classified as true or false. They are mostly empty headlines. Do not include\n",
    "- **1 (33 - 60 characters)** -> Entries seem very reliable and the ones with the best score show most potential. Include best 25%\n",
    "- **2 (61 - 88 characters)** -> These also show promise and look reliable upon initial inspection. Include best 25%\n",
    "- **3 (89 - 116 chatacters)** -> Same with these. Include best 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d42685d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for category in [1,2,3]:\n",
    "    category_df = Auto_false_score[Auto_false_score['char__category'] == category].sort_values('adjusted_leven', ascending=False)\n",
    "    dfs.append(category_df.head(int(len(category_df) * (0.20))))\n",
    "    \n",
    "Final_Auto_False = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f936b7",
   "metadata": {},
   "source": [
    "## 6. Saving Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9afafd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch the tweet_fake labels to reflect not wether the tweet is fake but wether if it's true\n",
    "Final_Auto_False = Final_Auto_False.assign(tweet__fake=0)\n",
    "Final_Auto_True = Final_Auto_True.assign(tweet__fake=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44894d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging True and False datasets together\n",
    "Final_df = pd.concat([Final_Auto_False, Final_Auto_True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bd43d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unncessery columns, renamining and rearranging remaining one\n",
    "Final_df = Final_df.sample(frac=1).reset_index(drop=True).drop(['leven__score', 'tweet__count', 'adjusted_leven', 'char__category'], axis=1)\n",
    "Final_df.rename(columns={'tweet__fake': 'claim_veracity', 'tweet__text': 'claim'}, inplace=True)\n",
    "Final_df = Final_df[['claim', 'claim_veracity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "121f82d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_df.to_csv('TwitterFakeNews_Picked.csv', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
